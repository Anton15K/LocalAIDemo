spring.application.name=LocalAIDemo

# ===========================================
# Database Configuration (PostgreSQL + pgvector)
# ===========================================
spring.datasource.url=jdbc:postgresql://localhost:5432/mydatabase
spring.datasource.username=myuser
spring.datasource.password=secret
spring.datasource.driver-class-name=org.postgresql.Driver

# JPA / Hibernate
spring.jpa.hibernate.ddl-auto=validate
spring.jpa.show-sql=true
spring.jpa.properties.hibernate.format_sql=true
spring.jpa.properties.hibernate.dialect=org.hibernate.dialect.PostgreSQLDialect

# ===========================================
# Flyway Database Migrations
# ===========================================
spring.flyway.enabled=true
spring.flyway.locations=classpath:db/migration
spring.flyway.baseline-on-migrate=true

# ===========================================
# Ollama Configuration
# ===========================================
spring.ai.ollama.base-url=http://localhost:11434
spring.ai.ollama.chat.options.model=mistral
spring.ai.ollama.chat.options.temperature=0.7
spring.ai.ollama.embedding.options.model=nomic-embed-text

# ===========================================
# PGVector Store Configuration
# ===========================================
spring.ai.vectorstore.pgvector.index-type=HNSW
spring.ai.vectorstore.pgvector.distance-type=COSINE_DISTANCE
spring.ai.vectorstore.pgvector.dimensions=768

# ===========================================
# Server Configuration
# ===========================================
server.port=8080

# ===========================================
# Logging
# ===========================================
logging.level.com.Anton15K.LocalAIDemo=DEBUG
logging.level.org.springframework.ai=DEBUG

# ===========================================
# Hugging Face / Dataset Ingestion
# ===========================================
# Token should be provided via env var (e.g., APP_HUGGINGFACE_TOKEN) and never committed.
app.huggingface.token=

# Auto-ingest DeepMath-103K on startup (recommended to enable in Docker only)
app.ingestion.deepmath.enabled=false
app.ingestion.deepmath.dataset=zwhe99/DeepMath-103K
app.ingestion.deepmath.config=default
app.ingestion.deepmath.split=train

# Paging controls
# Note: Hugging Face datasets-server enforces /rows length <= 100.
app.ingestion.deepmath.batch-size=100
# Set >0 to limit ingestion for local dev (0 = ingest all)
app.ingestion.deepmath.max-rows=1000

# Embedding indexing is expensive for 103k rows; keep false unless you really want it.
app.ingestion.deepmath.index-embeddings=false

# Throttling / back-pressure
# Delay after each Hugging Face /rows page fetch+import (ms)
app.ingestion.deepmath.request-delay-ms=250
# When indexing embeddings, index in smaller chunks to avoid overloading Ollama/DB
app.ingestion.deepmath.index-chunk-size=100
# Delay between indexing chunks (ms)
app.ingestion.deepmath.index-delay-ms=200

# ===========================================
# Lecture Theme Extraction (LLM)
# ===========================================
# Limits to keep LLM prompt size and CPU usage under control.
app.theme-extraction.max-topics-in-prompt=120
app.theme-extraction.max-transcript-chars=12000
app.theme-extraction.max-themes=8
