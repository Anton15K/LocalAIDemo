spring.application.name=LocalAIDemo

# ===========================================
# Database Configuration (PostgreSQL + pgvector)
# ===========================================
spring.datasource.url=jdbc:postgresql://localhost:5432/mydatabase
spring.datasource.username=myuser
spring.datasource.password=secret
spring.datasource.driver-class-name=org.postgresql.Driver

# JPA / Hibernate
spring.jpa.hibernate.ddl-auto=validate
spring.jpa.show-sql=true
spring.jpa.properties.hibernate.format_sql=true
spring.jpa.properties.hibernate.dialect=org.hibernate.dialect.PostgreSQLDialect

# ===========================================
# Flyway Database Migrations
# ===========================================
spring.flyway.enabled=true
spring.flyway.locations=classpath:db/migration
spring.flyway.baseline-on-migrate=true

# ===========================================
# Ollama Configuration
# ===========================================
spring.ai.ollama.base-url=http://localhost:11434
# Theme extraction / topic selection is closer to classification than creative writing;
# lower temperature reduces "topic drift" (e.g., Geometry vectors -> Linear Algebra).
spring.ai.ollama.chat.options.model=deepseek-v3.1:671b-cloud
#spring.ai.ollama.chat.options.model=mistral
spring.ai.ollama.chat.options.temperature=0.5
# Recommended (optional) chat models that usually classify math topics better than mistral:
# spring.ai.ollama.chat.options.model=qwen2.5-math:7b
# spring.ai.ollama.chat.options.model=qwen2.5:14b
# Embeddings: changing this requires reindexing the pgvector table (vector_store),
# otherwise semantic search will mix old/new embeddings.
# Keep a 768-dim embedding model unless you also migrate db/migration/V2__create_vector_store.sql.
spring.ai.ollama.embedding.options.model=nomic-embed-text
# Fallback if your Ollama registry uses a different tag:
# spring.ai.ollama.embedding.options.model=bge-base-en

# ===========================================
# PGVector Store Configuration
# ===========================================
spring.ai.vectorstore.pgvector.index-type=HNSW
spring.ai.vectorstore.pgvector.distance-type=COSINE_DISTANCE
spring.ai.vectorstore.pgvector.dimensions=768

# ===========================================
# Server Configuration
# ===========================================
server.port=8080
spring.servlet.multipart.max-file-size=500MB
spring.servlet.multipart.max-request-size=500MB
server.tomcat.max-swallow-size=500MB

# ===========================================
# Logging
# ===========================================
logging.level.com.Anton15K.LocalAIDemo=DEBUG
logging.level.org.springframework.ai=DEBUG

# ===========================================
# Hugging Face / Dataset Ingestion
# ===========================================
# Token should be provided via env var (e.g., APP_HUGGINGFACE_TOKEN) and never committed.
app.huggingface.token=

# Auto-ingest DeepMath-103K on startup (recommended to enable in Docker only)
app.ingestion.deepmath.enabled=false
app.ingestion.deepmath.dataset=zwhe99/DeepMath-103K
app.ingestion.deepmath.config=default
app.ingestion.deepmath.split=train

# Paging controls
# Note: Hugging Face datasets-server enforces /rows length <= 100.
app.ingestion.deepmath.batch-size=100
# Set >0 to limit ingestion for local dev (0 = ingest all)
app.ingestion.deepmath.max-rows=2000

# Embedding indexing is expensive for 103k rows; keep false unless you really want it.
app.ingestion.deepmath.index-embeddings=true

# Throttling / back-pressure
# Delay after each Hugging Face /rows page fetch+import (ms)
app.ingestion.deepmath.request-delay-ms=250
# When indexing embeddings, index in smaller chunks to avoid overloading Ollama/DB
app.ingestion.deepmath.index-chunk-size=100
# Delay between indexing chunks (ms)
app.ingestion.deepmath.index-delay-ms=200

# ===========================================
# Lecture Theme Extraction (LLM)
# ===========================================
# Limits to keep LLM prompt size and CPU usage under control.
app.theme-extraction.max-topics-in-prompt=120
app.theme-extraction.max-transcript-chars=12000
app.theme-extraction.max-themes=8
# Chunk-level theme extraction (for 1.5-3h lectures, chunk ~10 min)
# Optimal Values Logic:
# - chunk-size-words=1500: ~7-8 mins. Good balance between context and granularity.
# - min-chunk-occurrences=2: Ensures theme isn't a "one-off" mention.
# - min-occurrence-ratio=0.15: Dynamic threshold. For 3h (18 chunks), requires ~3 chunks.
# - max-themes-per-chunk=5: Allow LLM to find all relevant topics in the segment.
# - max-final-themes=10: Focused summary, but broad enough for a long lecture.

# now determined dynamically based on lecture length
app.theme-extraction.chunk-level-enabled=true
app.theme-extraction.chunk-size-words=1500
app.theme-extraction.min-chunk-occurrences=2
app.theme-extraction.min-occurrence-ratio=0.15
app.theme-extraction.max-themes-per-chunk=5
app.theme-extraction.max-final-themes=10

# ===========================================
# Topic Mapping (post-process LLM themes)
# ===========================================
# Ensures mappedTopic is always one of the existing DB topics.
app.topic-mapping.cache-ttl-seconds=600
app.topic-mapping.max-candidates=5000
app.topic-mapping.min-score=3

# ===========================================
# Problem Retrieval
# ===========================================
app.retrieval.topic-match-base-score=0.6

# ===========================================
# Assembly AI Configuration
# ===========================================
# API Key should be provided via environment variable: APP_ASSEMBLYAI_API_KEY
app.assemblyai.api-key=${APP_ASSEMBLYAI_API_KEY:}
app.assemblyai.base-url=https://api.assemblyai.com/v2
openai.api.key=${OPENAI_API_KEY:}